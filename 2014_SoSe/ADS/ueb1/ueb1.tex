\documentclass[11pt,a4paper,ngerman]{article}
\usepackage[bottom=2.5cm,top=2.5cm]{geometry}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{fancyref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{stmaryrd}
\usepackage{paralist}

\usepackage[pdftex, bookmarks=false, pdfstartview={FitH}, linkbordercolor=white]{hyperref}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead[C]{ADS}
\fancyhead[L]{Übung 1}
\fancyhead[R]{SoSe 2014}
\fancyfoot{}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage \hspace{1px} of \pageref{LastPage}}
\renewcommand{\footrulewidth}{0.5pt}
\renewcommand{\headrulewidth}{0.5pt}
\newcommand{\set}[1]{ \{ #1 \}}
\setlength{\parindent}{0pt}
\setlength{\headheight}{0pt}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\Rarr}{\Rightarrow}
\newcommand{\rarr}{\rightarrow}
\newcommand{\Pot}{\mathcal{P}}
\newcommand{\abs}[1]{\left |#1\right|}
\newcommand{\solved}{$\mbox{}$ \hfill $\square$}
\newcommand{\Epsilon}{\mathcal{E}}

\newcommand{\erw}[1]{\text{\bfseries E} \left[ #1 \right]}
\newcommand{\prob}[1]{\text{Pr}\left[ #1 \right]}

\date{}
\title{Übung 12}
\author{Max Wisniewski, Melanie Skodzik}


%%
%% Enviroments for proofs and lemmas
%%
\newtheorem{prop}{\bfseries Behauptung}
\newtheorem{lemma}{\bfseries Lemma}

\begin{document}

\lstset{language=Pascal, basicstyle=\ttfamily\fontsize{10pt}{10pt}\selectfont\upshape, commentstyle=\rmfamily\slshape, keywordstyle=\rmfamily\bfseries, breaklines=true, frame=single, xleftmargin=3mm, xrightmargin=3mm, tabsize=2, mathescape=true}

\renewcommand{\figurename}{Figure}

\maketitle
\thispagestyle{fancy}


\subsection*{Aufgabe 1}

Angenommen, wir haben eine Hashtabelle mit $n$ Plätzen, die eine Menge $S \subseteq K$ der Größe $n$ speichert. Konflikte werden mit Hilfe von Verkettung gelöst. Wir nehmen an, dass sich die Hashfunktion wie eine zufällige Funktion verhält.

Für einen Platz $i$ in der Hashtabelle, bezeichne mit $Q_i$, die \emph{Anzahl der Schlüssel}, die auf $i$ gehasht werden. Unser Ziel ist es 
$\erw{max_{i=0}^{n-1} Q_i}$ zu bestimmen, also den Erwartungswert für die maximale Anzahl an Elementen, die auf denselben Platz abgebildet werden.

\subsubsection*{(a)}

Begründen Sie in einem Satz: Es gilt für $i = 0, \dots n-1$ und $ = 0 , \dots, n$,
$$
	\prob{Q_i = r} = \left(\frac{1}{n} \right)^r \left( 1 - \frac{1}{n}\right)^{n-r} \binom{n}{r}.
$$

\noindent\textbf{Begründung:}\\
Die Hashfunktion verhält sich wie eine zufällige Funktion auf dem Array der Größe $n$, womit jedes Element die Wahrscheinlichkeit $\frac{1}{n}$ hat auf ein bestimmten Platz gehasht zu werden, und damit $r$ Elemente in einem fixierten Platz $i$ sind müssen eben $r$ mal dieses Ereignis eintreten und $n-r$ mal das Gegenereignis, wobei wir aus den $n$ Elementen die gehasht werden beliebige $r$ nehmen können und diese Ereignisse unterscheiden sich auch, da wir die Elemente unterscheiden können.

\subsubsection*{(b)}

Zeigen Sie,

$$
	\prob{max_{i=0}^{n-1} Q_i = r} \leq n \cdot \prob{Q_0 = r}.
$$

\noindent\textbf{Beweis:}\\
Damit das Maximum von allen gleich $r$ ist, muss mindestens ein Platz $r$ Elemente haben und alle anderen höchstens $r$, daher gilt das folgende
$$\begin{array}{rcl}
	\prob{max_{i=0}^{n-1} Q_i = r} &=& \prob{\bigvee_{i=0}^{n-1} \left( Q_i = r \wedge \bigwedge_{j=0, j \not= i}^{n-1} Q_j \leq r \right)}\\
		&\leq& \sum_{i=0}^{n-1} \prob{Q_i = r} \cdot \prod_{j=0, j \not= i}^{n-1} \prob{Q_j \leq r}\\
		&\stackrel{|\prob{.}|\leq 1}{\leq} & \sum_{i=0}^{n-1} \prob{Q_i = r}\\
		&\stackrel{(a)}{=}& \sum_{i=0}^{n-1} \prob{Q_0 = r}\\
		&=& n \cdot \prob{Q_0 = r}.
\end{array}$$

\subsubsection*{(c)}

Zeigen Sie $\prob{Q_0 = r} \leq e^r / r^r$.\\

\noindent\textbf{Beweis:}

$$\begin{array}{rcl}
	\prob{Q_0 = r} &\stackrel{(a)}{=}&  \left(\frac{1}{n} \right)^r \left( 1 - \frac{1}{n}\right)^{n-r} \binom{n}{r}\\
		&\stackrel{\text{Stirling}}{=}&  \left(\frac{1}{n} \right)^r \left( 1 - \frac{1}{n}\right)^{n-r} \left( \frac{ne}{r} \right)^r\\
		&=& \frac{(n-1)^{n-r}n^re^r}{n^r n^{n-r} r^r}\\
		&\leq& \frac{n^{n-r}n^re^r}{n^nr^r}\\
		&=& \frac{e^r}{r^r}.
\end{array}$$

\subsubsection*{(d)}

Definiere $r_0 := c \log \, n / \log \log \, n$, für eine Konstante $c > 1$. Zeigen Sie, dass man $c$ so wählen kann, dass $\prob{Q_0 = r} < 1 / n^3$ ist, für alle $r \geq r_0$.\\
Folgern Sie daraus:
$$
	\prob{max_{i=0}^{n-1} Q_i \geq r_0} \leq \frac{1}{n}.
$$

\noindent\textbf{Beweis:}\\

Sei $r_0$ wie in der Aufgabe und $r \geq r_0$.\\

Zunächst beobachten wir, dass $r^{-r}$ eine Nullfolge ist und streng monoton fallenend. Daher wird das maximum in $r = r_0$ angenommen.

$$\begin{array}{rcl}
	\prob{Q_0 = r} &=& e^r \cdot r^{-r}\\
			&\leq& e^{r_0} \cdot r_0^{-r_0}\\
			&=& (e / r_0)^{r_0}\\
			&=& \left(\frac{e \cdot \log \log \, n}{c \log n}\right)^\frac{c \log \, n}{\log \log \, n}\\
\end{array}$$

Wir zeigen mit dieser Gleichung nun, dass wir das gewünschte Ziel erreichen können.
$$\begin{array}{crcl}
	&\left(\frac{e\cdot \log \log \, n}{c \cdot \log \, n}\right)^\frac{c \cdot \log n}{\log \log n} &<& \frac{1}{n^3}\\
\Leftrightarrow & \frac{e\log \log \, n}{c\log \, n} &<& \left(\frac{1}{n}\right)^\frac{3\log \log \, n}{c\log n}\\
\Leftrightarrow & \frac{e\log \log \, n}{c\log \, n} &<& \left(e^{- \log n} \right)^\frac{3\log \log \, n}{c\log n}\\
\Leftrightarrow & \frac{e\log \log \, n }{c\log \, n} &<& \left(e^{- \log \log \, n}\right)^\frac{3}{c}\\
\Leftrightarrow & \frac{e\log \log \, n}{c\log \, n} &<& \frac{1}{\left(\log \, n\right)^\frac{3}{c}}\\
\Leftrightarrow & e \log \log \, n &<& \frac{c \log \, n}{(\log n)^\frac{3}{c}}\\
\Leftrightarrow & e \log \log \, n &<& c (\log \, n)^{1 - \frac{3}{c}}
\end{array}$$

Auf der llinken Seite haben wir nun den Logarithmus von $\log \, n$ zu stehen und auf der rechten Seite $1-\frac{3}{c}$ zu stehen. Wir wissen, dass der Logarithmus langsamer wächst als jede Wurzel oder Potenzfunktion, also müssen wir nur sicher gehen, dass die rechte Seite wirklich wächst. Dafür muss die Potenz nämlich größer als $0$ sein. Demnach muss unser $c>3$ sein. Da unser Vorfaktor damit auch größer als $e$ ist, können wir ein beliebiges $c > 3$ nehmen. Da wir unbeschränkt sind, können wir, falls es für die ersten Folgenglieder noch nicht gilt unser $c$ so groß wählen, dass es passt, da wir ja wissen, dass es eine Konstante gibt (und damit auch einen konstanten Wert) ab dem die Wurzelfunktion größer ist als der Logarithmus.

Dieses Ergebnis können wir verwenden um die nächste Abschätzung zu machen.
$$\begin{array}{rcl}
	\prob{\max_{i=0}^{n-1} Q_i \geq r_0} &=& \prob{\underset{r \geq r_0}{\bigvee} \max_{i=0}^{n-1} Q_i = r}\\
		&=& \underset{r\geq r_0}{\sum} \prob{\max_{i=0}^{n-1} Q_i = r}\\
		&\stackrel{(b)}{\leq}& \underset{r\geq r_0}{\sum} n \cdot \prob{Q_0 = r}\\
		&<& \underset{r \geq r_0}{\sum} n \cdot \frac{1}{n^3}\\
		&\leq& \underset{r \geq 0}{\sum} n \cdot \frac{1}{n^3}\\
		&=& n \cdot n \cdot \frac{1}{n^3} = \frac{1}{n}
\end{array}$$

\subsubsection*{(e)}
Zeigen Sie
$$
	\erw{\max_{i=0}^{n-1} Q_i} \leq r_0 \cdot \prob{\max_{i=0}^{n-1} Q_i < r_0} + n \cdot \prob{\max_{i=0}^{n-1} Q_i \geq r_0}.
$$

und folgern Sie daraus $\erw{\max_{i=0}^{n-1}Q_i} = \mathcal{O}(\log \, n / \log \log \, n)$.

\noindent\textbf{Beweis:}\\

Der diskrete Erwartungswert hierfür ist definiert als
$$
	\erw{\max_{i=0}^{n-1} Q_i} = \sum_{r=0}^n r \cdot \prob{\max_{i=0}^{n-1} Q_i = r}
$$
wobei wir die Summe bei $r_0$ in zwei Hälften teilen können
$$
	\erw{\max_{i=0}^{n-1} Q_i} = \sum_{r=0}^{r_0-1}r \cdot \prob{\max_{i=0}^{n-1} Q_i = r} + \sum_{r= r_0}^n  r \cdot \prob{\max_{i=0}^{n-1} Q_i = r}
$$
Nun können wir in beiden Summen die Faktoren nach oben abschätzen.
$$
	\erw{\max_{i=0}^{n-1} Q_i} \leq \sum_{r=0}^{r_0-1}r_0 \cdot \prob{\max_{i=0}^{n-1} Q_i = r} + \sum_{r= r_0}^n  n \cdot \prob{\max_{i=0}^{n-1} Q_i = r}
$$
Nun erhalten wir genau das heraus, was wir wollten, da wir jetzt über die gesammten Intervalle summieren (wichtig ist,
dass die rechten Seiten gleich bleiben, da die Ereignisse disjunkt sind).
$$
	\erw{\max_{i=0}^{n-1}Q_i} \leq r_0 \prob{max_{i=0}^{n-1} Q_i < r_0} + n \cdot \prob{max_{i=0}^{n-1} Q_i \geq r_0}
$$

Nun können wir das ganze schnell umformen.
$$\begin{array}{rcl}
	\erw{\max_{i=0}^{n-1}Q_i} &\leq&  r_0 \prob{max_{i=0}^{n-1} Q_i < r_0} + n \cdot \prob{max_{i=0}^{n-1} Q_i \geq r_0}\\
		&=& r_0 (1 - \prob{max_{i=0}^{n-1} Q_i \geq r_0}) + n \cdot \prob{max_{i=0}^{n-1} Q_i \geq r_0}\\
		&=& r_0  + (n - r_0) \cdot  \prob{max_{i=0}^{n-1} Q_i \geq r_0}\\
		&\leq& r_0 + n \cdot  \prob{max_{i=0}^{n-1} Q_i \geq r_0}\\
		&\stackrel{(d)}{\leq}& r_0 + n \cdot \frac{1}{n}\\
		&=& r_0 + 1 =  \mathcal{O} (\log \, n / \log \log \, n)
\end{array}$$

\mbox{}\hfill$\square$

\subsection*{Aufgabe 2}

Eine Varianete der amortisierten Analyse verwendet so genannte \emph{Potentialfunktionen}. Wir betrachten eine Datenstruktur $D$. Sei $\mathcal{D}$ die Menge aller möglichen Zustände von $D$. Eine Potentialfunktion $\Phi \, : \, \mathcal{D} \rightarrow \mathbb{N}_0$ ist eine Funktion, die jedem Zustand $D \in \mathcal{D}$ der Datenstruktur eine natürliche Zahl $\Phi(D)$ zuordnet. Für den ursprünglichen Zustand $D_0$ muss gelten $\Phi(D_0) = 0$. 

Wir definieren nun die \emph{amortisierten Kosten} einer Operation $X$ auf der Datenstruktur. Sei $D$ die Datenstruktur vor der Operation $X$ und $D'$ die Datenstruktur nach $X$. Seien $c_X$ die tatsächlichen Kosten von $X$. Dann sind die amortisierten Kosten von $X$, $\hat{c}_X := c_X + \Phi(D') - \Phi(D)$.

\subsubsection*{(a)}

Geben Sie eine Interpretation der Potentialfunktion und erklären Sie die Intuition hinter der obigen Definition der amortisierten Kosten.\\

\noindent\textbf{Lösung:}\\

Bei der amortisierten Analyse versuchen wir die Kosten einer Folge von Operationen auf die Einzelnen Operationen aufzuteilen, so dass im Durchschnitt jede Operation nicht zu lange dauert. Bei der Potentialfunktion handelt es sich um eine Funktion, die der Datenstruktur ein \emph{Guthaben} zuweist. Jede Operation kann nun entweder Guthaben hinzufügen oder vorhandenes Guthaben wegnehmen. Es kann allerdings nur schon vorhandenes verbraucht werden (Codomain ist $\mathbb{N}_0$), da Folgen, die nun genau nach dieser Operation enden nicht global ausgeglichen sind.

So kann man sich auch die Definition der amortisierten Kosten erklären. Sollte die Operation tatsächlich hohe kosten haben, so kann sie etwas vom Guthaben nehmen und so auf geringere Kosten kommen.
$\Phi(D') - \Phi(D) > 0 \Rightarrow \hat{c}_X < c_X$. Die Datenstruktur hat dadurch aber natürlich weniger Guthaben.

Sollte eine Operation sehr geringe Kosten haben, kann Sie Guthaben hinterlegen. Die amortisierten Kosten sind dann für diese Operation höher,
aber die Datenstruktur hat mehr zum Abheben. $\hat(c)_X > c_X \Rightarrow \Phi(D) < \Phi(D')$.

\subsubsection*{(b)}

Sei $X_1, X_2, \dots, X_n$ eine Folge von Operationen, die auf der initialen Datenstruktur $D_0$ ausgeführt wird. Zeigen Sie, dass 
$\sum_i c_{X_i} \leq \sum_i \hat{c}_{X_i}$ ist. Interpretieren Sie das Ergebnis.\\

\noindent\textbf{Beweis:}\\
Wir formen zunächst nach definition der amortisierten Kosten um.
$$
	\sum_i c_{X_i} = \sum_i \hat{c}_{X_i} + \Phi(D_i) - \Phi(D_{i-1}) = \Phi(D_n) - \Phi(D_0) + \sum_i \hat{c}_{X_i} \leq \sum_i \hat{c}_{X_i}
$$
Der letzte Schritt gilt, da $\Phi(D_0) = 0$ und $\Phi(D_n) \geq 0$ da dies genau der Bildbereich ist.\\

Dies sollte auch intuitiv klar sein, da jede Operation etwas auf dem Konto hinterlegen kann. Nach dem alle Operationen ausgeführt worden sind, kann also immer noch etwas auf dem Konte sein, wodurch die amortiesierte Folge schwerer erscheint. Dies ist aber auch gut so, denn sollte das Guthaben nach jeder Folge auf $0$ zurückgehen, so würden die amortisierten Kosten gleich den tatsächlichen Kosten sein.

\subsubsection*{(c)}

Ein Binärzählerk ist eine Datnstruktur, die einen Binärstring der Länge $n$ speicherert und nur eine einzige Funktionuntertützt, \emph{increment}, die den gespeicherten Wert um genau eins erhöht. Die Kosten sind dabei dei Anzahl der Bits, die geändert werden.

Sei $\Phi$ die Funktion, die einen Zustand des Binärzählwerks die Anzahl der Einsen im aktuellen Zählerstand zuordnet.

Verwenden Sie $\Phi$, um zu zeigen, dass die amortisierten Kosten pro Zählvorgang kosntant sind. Folgern Sie, dass das Binärzählwerk mit Kosten $O(n)$ von $0$ bis $n$ zählen kann.\\

\noindent\textbf{Beweis:}\\
Da wir nur eine einzige Operation haben, besteht die Folge aus $n$ Operation nur aus \emph{increment}. Sei $X$ eine beliebige \emph{increment} Operation, sowie $D'=a_n'\dots a_1'$ der Zustand nach dem ausführen von $X$ und $D=a_n\dots a_1$ der Zustand vor dem ausführen. Da $X$ ein inkrement auf $D$ ist, interessieren wir uns für das folgende: Solange $D$ nicht maximal ist ($2^n$) sei $i$ die Stelle mit $a_i = 0$ und $\forall j < i \, . \, a_j = 1$, also die erste Stelle die gleich $0$ ist. Die Operation $X$ tut nun das folgende:
$$\begin{array}{rclr}
	a_x' &=& a_x\qquad \qquad &  x > i\\
	a_x' &=& 1 & x = i\\
	a_x' &=& 0 & x < i
\end{array}$$

Sehen wir uns die tatsächlichen Kosten von $X$ an, so sind dies $c_X = 1 + (i-1) = i$, wir setzen die $i$-te Stelle auf $1$ und alle kleineren auf $0$.
Sehen wir uns die Datenstruktur an. Ist $w_D$ das Guthaben der Datenstruktur vorher, so ist $w_{D'} = w_D - (i-1) + 1 = w_D - i +2$ der Wert danach, da wir die ersten $i-1$ Stellen von $1$ auf $0$ ändern, den Wert also veringern, aber die $i$-te Stelle von $0$ auf $1$ ändern. Der Rest bleibt gleich.

Die amortisierten Kosten der Operation sind daher
$$
	\hat{c}_X = c_X + \Phi(D') - \Phi(D) = i + \Phi(D) -  i  + 2 - \Phi(D) = 2
$$
und somit ist jede Operation konstant.

Wir können noch sehen, dass $\Phi$ die Anforderungen erfüllt, da der Zähler zu beginn $0$ ist, also $\Phi(0) = 0$ gilt.\\

Nun kostet eine Folge von $n$ \emph{increment} Operationen genau
$$
	\sum_{i=1}^n c_{X_i} \stackrel{(b)}{\leq} \sum_{i=1}^n \hat{c}_{X_i} = \sum_{i=1}^n 2 = 2n = \mathcal{O}(n)
$$

\label{LastPage}
\end{document}
